{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "This notebook helps perform the feature importance assessment. To start working with this notebook, you need to download the [IEEE dataset](https://ieee-dataport.org/open-access/experimental-database-detecting-and-diagnosing-rotor-broken-bar-three-phase-induction), unpack it and place in the folder named **IEEE** above the current folder, containing the notebook. Also, you should have the libraries from *requirements.txt* installed in the virtual environment, as described in the README file. Next, run the first two stages, Features and Data, to create the training and testing datasets with needed FFT window parameters in **Data** folder.\n",
    "\n",
    "The notebook consists of the three main parts:\n",
    "\n",
    "- [Model training](#model-training)\n",
    "- [Permutation importance](#model-training)\n",
    "- [SHAP](#shap)\n",
    "\n",
    "First, the models are trained with the given set of hyperparameters, as is in the ML pipelines. Next, permutation importance and SHAP values are calculated for the existing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use DVC to get data with required dropped loading levels, then load the .csv files here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=pd.read_csv('Data/processed/X_test.csv')\n",
    "y_test=pd.read_csv('Data/processed/y_test.csv')\n",
    "X_train=pd.read_csv('Data/processed/X_train.csv')\n",
    "y_train=pd.read_csv('Data/processed/y_train.csv')\n",
    "dataset=pd.read_csv('Data/raw/dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots()\n",
    "corr=dataset.drop(columns=['Loading','Label']).corr(\"pearson\")\n",
    "sns.heatmap(corr,mask=np.zeros_like(corr, dtype=bool),\n",
    "            cmap=sns.color_palette(\"coolwarm\", as_cmap=True),\n",
    "            square=True, ax=ax)\n",
    "ax.set_title('Feature correlation, Pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={\n",
    "\"SVC__C\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "\"SVC__gamma\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "pipe=Pipeline(steps=[('scaler', StandardScaler()),\n",
    "                             ('SVC', SVC(probability=True,decision_function_shape='ovr'))])\n",
    "clf=GridSearchCV(estimator=pipe,\n",
    "                     param_grid=param_grid,\n",
    "                     cv=5,\n",
    "                     scoring='neg_log_loss',\n",
    "                     return_train_score=True,\n",
    "                     verbose=1,\n",
    "                     n_jobs=4)\n",
    "clf.fit(X_train,np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model=clf.best_estimator_\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_f1=f1_score(y_test,svc_model.predict(X_test),average='macro')\n",
    "svc_log_loss=log_loss(y_test,svc_model.predict_proba(X_test))\n",
    "print(f\"SVC model F1 = {svc_f1}\\nSVC model log-loss = {svc_log_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_param_grid={\n",
    "    \"GBC__n_estimators\": [200, 500],\n",
    "    \"GBC__max_depth\": [6,9],\n",
    "    \"GBC__learning_rate\": [0.1],\n",
    "    \"GBC__subsample\": [0.3],\n",
    "    \"GBC__validation_fraction\": [0.2],\n",
    "    \"GBC__tol\": [0.01, 0.1],\n",
    "    \"GBC__n_iter_no_change\": [50],\n",
    "    \"GBC__random_state\": [0],\n",
    "}\n",
    "gbc_pipe=Pipeline(steps=[('scaler', StandardScaler()),\n",
    "                             ('GBC', GradientBoostingClassifier())])\n",
    "clf_gbc=GridSearchCV(estimator=gbc_pipe,\n",
    "                     param_grid=gbc_param_grid,\n",
    "                     cv=5,\n",
    "                     scoring='neg_log_loss',\n",
    "                     return_train_score=True,\n",
    "                     verbose=1,\n",
    "                     n_jobs=4)\n",
    "clf_gbc.fit(X_train,np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_model=clf_gbc.best_estimator_\n",
    "clf_gbc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_f1=f1_score(y_test,gbc_model.predict(X_test),average='macro')\n",
    "gbc_log_loss=log_loss(y_test,gbc_model.predict_proba(X_test))\n",
    "print(f\"GBC model F1 = {gbc_f1}\\nGBC model log-loss = {gbc_log_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model=keras.Sequential([\n",
    "    layers.BatchNormalization(name='Layer_1',input_shape=[np.shape(X_train)[1]]),\n",
    "    layers.Dense(name='Layer_2',units=128,activation='swish',kernel_regularizer=regularizers.L2(0.01)),\n",
    "    layers.Dense(name='Layer_3',units=128,activation='selu',kernel_regularizer=regularizers.L2(0.001)),\n",
    "    layers.BatchNormalization(name='Layer_4',),\n",
    "    layers.Dropout(0.3,name='Layer_5'),\n",
    "    layers.Dense(name='Layer_6',units=128,activation='swish',kernel_regularizer=regularizers.L2(0.001)),\n",
    "    layers.Dense(name='Output_layer',units=5,activation='softmax'),])\n",
    "mlp_model.compile(\n",
    "    \n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss='SparseCategoricalCrossentropy',\n",
    "    metrics=['SparseCategoricalAccuracy',\n",
    "             'SparseCategoricalCrossentropy'],\n",
    "    jit_compile=True\n",
    ")\n",
    "#training with early stoppings defined\n",
    "early_stoppings=keras.callbacks.EarlyStopping(patience=10,\n",
    "min_delta=0.001,restore_best_weights=True,start_from_epoch=25)\n",
    "history=mlp_model.fit(X_train,y_train,\n",
    "                  validation_data=(X_test,y_test),\n",
    "                  batch_size=16,\n",
    "                  epochs=128,\n",
    "                  callbacks=[early_stoppings],\n",
    "                  verbose=1,\n",
    "                  use_multiprocessing=True,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score\n",
    "mlp_y_hat=mlp_model.predict(X_test)\n",
    "dl_y_hat=np.argmax(mlp_y_hat,axis=1)\n",
    "dl_f1=f1_score(y_test,dl_y_hat,average='macro')\n",
    "dl_log_loss=log_loss(y_test,mlp_model(X_test))\n",
    "print(f\"MLP model F1 = {dl_f1}\\nMLP model log-loss = {dl_log_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_perm = PermutationImportance(svc_model, random_state=0).fit(X_test, np.ravel(y_test))\n",
    "eli5.show_weights(svc_perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_perm = PermutationImportance(gbc_model, random_state=0).fit(X_test, np.ravel(y_test))\n",
    "eli5.show_weights(gbc_perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(estimator,x,y):\n",
    "    dl_y_hat=estimator.predict(x)\n",
    "    dl_y_hat=np.argmax(dl_y_hat,axis=1)\n",
    "    dl_f1=f1_score(y,dl_y_hat,average='macro')\n",
    "    return dl_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_multi = permutation_importance(mlp_model, X_test, y_test, n_repeats=30, random_state=0, scoring=scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_perm=pd.DataFrame([X_test.columns,r_multi['importances_mean'],r_multi['importances_std']],index=['Feature','Permutation Importance','Std of importance'])\n",
    "mlp_perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "svc_explainer = shap.KernelExplainer(svc_model.predict_proba,shap.kmeans(X_train,5),seed=0)\n",
    "svc_shap_values = svc_explainer.shap_values(X_test)\n",
    "shap.force_plot(svc_explainer.expected_value[0], svc_shap_values[0], X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(svc_shap_values[0], X_test, show=False)\n",
    "plt.title('Inlfuence of features on 0BRB prediction of SVM model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(svc_explainer.expected_value[0], svc_shap_values[0], X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "gbc_explainer = shap.KernelExplainer(gbc_model.predict_proba,shap.kmeans(X_train,20),seed=0)\n",
    "gbc_shap_values = gbc_explainer.shap_values(X_test)\n",
    "shap.force_plot(gbc_explainer.expected_value[0], gbc_shap_values[0], X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(gbc_shap_values[3], X_test, show=False)\n",
    "plt.title('Inlfuence of features on 3BRB prediction of GBM model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "mlp_explainer = shap.KernelExplainer(mlp_model,shap.kmeans(X_train,20),seed=0)\n",
    "mlp_shap_values = mlp_explainer.shap_values(X_test)\n",
    "shap.force_plot(mlp_explainer.expected_value[0], mlp_shap_values[0], X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(mlp_shap_values[0], X_test, show=False)\n",
    "plt.title('Inlfuence of features on 0BRB prediction of MLP model')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
